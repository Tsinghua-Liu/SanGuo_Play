{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 二十个问题 游戏提示词"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "#在这里修改模型路径\n",
    "model_path = \"/mnt/2b44d1e2-569a-42b3-9610-04ea73771f19/liuqinghua/models/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28\"\n",
    "model_path = r\"D:\\model\\huggingface\\hub\\models--Qwen--Qwen2.5-7B-Instruct\\snapshots\\a09a35458c702b33eeacc393d103063234e8bc28\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#加载角色\n",
    "characters_list = {}\n",
    "with open(\"../characters.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    characters_list = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0addd942ca24766b1e2e5614e55b63e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=\"auto\",device_map = device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'zhangfei': {'image_path': 'gradio/image/张飞.png',\n  'lora_model': 'lora/output/Qwen2.5_instruct_lora_zhangfei/checkpoint-360',\n  'zh_name': '张飞'}}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "#创建 peft model 的个数要根据已经选择的人物进行选择\n",
    "peft_model_path =  peft_model_path = r\"output/Qwen2.5_instruct_lora_\"+ list(characters_list.keys())[0] +\"/checkpoint-360\"\n",
    "peft_model = PeftModel.from_pretrained(model,peft_model_path, adapter_name=list(characters_list.keys())[0])\n",
    "peft_model.eval()\n",
    "for name in list(characters_list.keys()):\n",
    "    peft_model_path = r\"output/Qwen2.5_instruct_lora_\"+name +\"/checkpoint-360\"\n",
    "    #peft_model_list.append(PeftModel.from_pretrained(model,peft_model_path, adapter_name=name))\n",
    "    peft_model.load_adapter(peft_model_path, adapter_name = name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(152064, 3584)\n        (layers): ModuleList(\n          (0-27): 28 x Qwen2DecoderLayer(\n            (self_attn): Qwen2SdpaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=3584, bias=False)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): Qwen2RotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=18944, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=18944, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=18944, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm()\n            (post_attention_layernorm): Qwen2RMSNorm()\n          )\n        )\n        (norm): Qwen2RMSNorm()\n      )\n      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n    )\n  )\n)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def generate_qwen7B(prompt ,lora=True):\n",
    "    if lora:\n",
    "        with torch.no_grad():\n",
    "            text = tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "            )\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "            generated_ids = peft_model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=1024,\n",
    "            )\n",
    "            generated_ids = [\n",
    "                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            return response\n",
    "    else:\n",
    "        with peft_model.disable_adapter():\n",
    "            text = tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "            )\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "            generated_ids = peft_model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=1024,\n",
    "\n",
    "            )\n",
    "            generated_ids = [\n",
    "                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            return response"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(152064, 3584)\n        (layers): ModuleList(\n          (0-27): 28 x Qwen2DecoderLayer(\n            (self_attn): Qwen2SdpaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=3584, bias=False)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): Qwen2RotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=18944, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=3584, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=18944, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n                (lora_dropout): ModuleDict(\n                  (zhangfei): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (zhangfei): Linear(in_features=18944, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (zhangfei): Linear(in_features=16, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm()\n            (post_attention_layernorm): Qwen2RMSNorm()\n          )\n        )\n        (norm): Qwen2RMSNorm()\n      )\n      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n    )\n  )\n)"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大地\n"
     ]
    }
   ],
   "source": [
    "def llm_generate_keyword( keyword = \"\"):\n",
    "    \"\"\"\n",
    "    :param  根据游戏规则为词语生成一个非常模糊的描述。\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"你需要为{keyword}打上一个非常模糊的类别标签（例如带电，纸质，不能吃，白色等宽泛的描述），你只能使用一个词语进行表述，字数少于4个字。描述当中不能与{keyword}有任何重复的字。\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    return generate_qwen7B(prompt=prompt,lora=False)\n",
    "\n",
    "def llm_generate_answer(question = \"\", keyword = \"\"):\n",
    "    \"\"\"\n",
    "    :param messages:  回答用户的问题： 是 否 不知道\n",
    "    :param character_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # unload adapter\n",
    "    prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"关键词：{keyword}。用户的问题是：{question}。\n",
    "        请你根据关键词来回答用户的问题。你只能回答'是'或者'否'或者'不确定'。\n",
    "        用户会提出一个与关键词相关的问题。例如关键词是：苹果，用户问题是：“它是红色的吗？” 你需要回答'否'，因为苹果不一定是红色的。用户问题是：“它是否长在树上？” 你需要回答'是'，因为苹果是长在树上的。用户问题是：“它一共有24个品种？” 你需要回答'不知道'，因为这可能超出你的认知了。\n",
    "\n",
    "        你只能回答'是'或者'否'或者'不知道'。\n",
    "        \"\"\"\n",
    "    }\n",
    "    ]\n",
    "    return generate_qwen7B(prompt,lora=False)\n",
    "\n",
    "def llm_generate_judge(predict = \"\", keyword = \"\"):\n",
    "    \"\"\"\n",
    "    :param messages:  回答用户的问题\n",
    "    :param character_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # unload adapter\n",
    "    prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"你正在主持一个游戏，你负责判断用户的猜测是否符合答案。如果用户的猜测与答案一致（或者是一个东西的不同表达），你需要回复'正确'，否则回复'错误'。如果已经非常靠近答案了，你需要回复'部分正确'。\n",
    "\n",
    "        例如：\n",
    "        答案词：优衣库衣服 。用户的猜测是：优衣库的衣服。  回复'正确'。\n",
    "        答案词：喜马拉雅山 。用户的猜测是：喜马拉雅。  回复'正确'。\n",
    "        答案词：中国国家图书馆 。用户的猜测是：图书馆。  回复'部分正确'。\n",
    "        答案词：王老吉 。用户的猜测是：加多宝。  回复'错误'。\n",
    "\n",
    "        下面请你判断用户的猜测，你只能回复'正确'、'错误'或者'部分正确'：\n",
    "        答案词：{keyword}。用户的猜测是：{predict}。\"\"\"\n",
    "    }\n",
    "    ]\n",
    "    return generate_qwen7B(prompt,lora=False)\n",
    "\n",
    "def llm_generate_character_question(character_name =\"zhangfei\",describe = \"\", history = \"\"):\n",
    "    \"\"\"\n",
    "    :param messages:  角色根据历史记录进行提问。\n",
    "    :param character_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # unload adapter\n",
    "    peft_model.set_adapter(character_name)\n",
    "    zh_name = characters_list[character_name][\"zh_name\"]\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"你需要扮演三国演义中的{zh_name}，假设你穿越到了古代，你和其他古代角色一起参与“二十问”游戏。你的最终目标是猜出属性包括'{describe}'的一个词语。\n",
    "            请参考其他古代角色的历史问答信息[{str(history) if history != \"\" else  describe}]，再提一个能用'是'或'否'回答的问题，确保这个问题能大幅缩小猜测的范围。不要重复已有角度，每次提问都要基于最新信息调整方向，目的是缩小答案的范围。\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    return generate_qwen7B(prompt)\n",
    "\n",
    "def llm_generate_character_predict(character_name =\"zhangfei\",describe = \"\", history = \"\"):\n",
    "    \"\"\"\n",
    "    :param messages:  角色根据历史记录进行猜测。\n",
    "    :param character_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # unload adapter\n",
    "    peft_model.set_adapter(character_name)\n",
    "    zh_name = characters_list[character_name][\"zh_name\"]\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"你需要扮演三国演义中的{zh_name}，你需要按照{zh_name}的风格发言，假设你穿越到了古代，你和其他古代角色一起参与“猜谜底”游戏。\n",
    "            谜底（是一个名词）属性包括'{describe}'。\n",
    "            请参考与这个谜底相关的历史问答信息[{str(history) if history != \"\" else  describe}]，逐步推理，猜测谜底应该是哪个词。\n",
    "            请你使用一个名词来回答。\"\"\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"role\": \"system\",\n",
    "        #     \"content\": f\"\"\"你需要扮演三国演义中的{zh_name}，你需要按照{zh_name}的风格发言，假设你穿越到了古代，你和其他古代角色一起参与“二十问”游戏。\n",
    "        #     你的最终目标是猜出属性包括'{describe}'的一个谜底（是一个名词）。\n",
    "        #     请参考与这个谜底相关的历史问答信息[{str(history) if history != \"\" else  describe}]，逐步推理，逐渐缩小猜测的范围。\n",
    "        #     下面请你开逐步思考，详细的展示思考过程，并且最终给出一个总结。\"\"\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"role\": \"user\",\n",
    "        #     \"content\":\"\"\"\n",
    "        #\n",
    "        #     \"\"\"\n",
    "        # }\n",
    "    ]\n",
    "    return generate_qwen7B(prompt)\n",
    "\n",
    "\n",
    "def test_():\n",
    "    messages = {}\n",
    "    messages[\"secret\"] = \"冰箱\"\n",
    "    messages[\"history\"] = \"\"\"第1轮：\n",
    "    \"\"\"\n",
    "    messages[\"vote\"] = \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "    response = character_llm_generate_vote(messages=messages, character_name=\"zhangfei\",character_list = ['刘备','关羽','曹操','诸葛亮'])\n",
    "    print(response)\n",
    "def test_2():\n",
    "    response = character_llm_generate_setter( character_name=\"zhangfei\",keyword=\"天宫空间站\")\n",
    "    print(response)\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    if True:\n",
    "        res = llm_generate_character_predict(describe=\"蓝色\",history=[\"玄德曰：此物可充饥否？判官曰：非也。云长曰：莫非飞禽走兽乎？判官曰：非也。翼德曰：此乃匠人所造之物乎？判官曰：非也。子龙曰：此物可与苍穹相系乎？判官曰：然也。孔明曰：此乃天地自然之象乎？判官曰：然也。云长曰：吾等肉眼可见乎？判官曰：然也。翼德曰：此物似云雾般无形乎？判官曰：非也。子龙曰：此物之巨，可遮天蔽日乎？判官曰：然也。玄德曰：此物有江河湖海乎？判官曰：然也。云长曰：此物上有生灵乎？判官曰：然也。翼德曰：此物昼夜交替乎？判官曰：然也。子龙曰：此物四季更迭乎？判官曰：然也。孔明曰：此物乃吾等立足之地乎？判官曰：然也。云长曰：此物环绕日轮乎？判官曰：然也。翼德曰：此物有月相伴乎？判官曰：然也。子龙曰：此物乃寰宇唯一乎？判官曰：非也。子龙曰：此物乃大地乎？判官曰：非也。\"])\n",
    "        print(res)\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "peft",
   "language": "python",
   "display_name": "PEFT"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}